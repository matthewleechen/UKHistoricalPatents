{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/UKHistoricalPatents/blob/main/KPSTBreakthroughs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides code to implement the breakthrough score calculations we use in **300 Years of British Patents**."
      ],
      "metadata": {
        "id": "horWK-DQlEpb"
      },
      "id": "horWK-DQlEpb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fab1ce3-62c7-4838-98db-75882c2d587c",
      "metadata": {
        "id": "1fab1ce3-62c7-4838-98db-75882c2d587c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import spacy\n",
        "from gensim.parsing.preprocessing import STOPWORDS as gensim_stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csr_matrix, vstack, save_npz, load_npz\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6457f2f8-8381-4173-b3b4-f559541b980e",
      "metadata": {
        "id": "6457f2f8-8381-4173-b3b4-f559541b980e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_text_length(example):\n",
        "    \"\"\"\n",
        "    gets total length of text across all pages\n",
        "    \"\"\"\n",
        "    total_length = sum(len(page['page_text']) for page in example['full_text'])\n",
        "\n",
        "    return {'text_length': total_length}\n",
        "\n",
        "\n",
        "\n",
        "def filter_dataset_by_length(dataset, min_chars=1000):\n",
        "\n",
        "    \"\"\"\n",
        "    keeps only dataset entries >= min_chars length\n",
        "    \"\"\"\n",
        "\n",
        "    # add length column\n",
        "    dataset = dataset.map(calculate_text_length)\n",
        "\n",
        "    # keep entries above min length\n",
        "    filtered_dataset = dataset.filter(lambda x: x['text_length'] >= min_chars)\n",
        "\n",
        "    # show filtering results\n",
        "    total = len(dataset)\n",
        "    filtered = len(filtered_dataset)\n",
        "    print(f\"original size: {total}\")\n",
        "    print(f\"filtered size: {filtered}\")\n",
        "    print(f\"removed: {total - filtered}\")\n",
        "\n",
        "    return filtered_dataset\n",
        "\n",
        "\n",
        "def load_all_stopwords():\n",
        "    \"\"\"\n",
        "    loads stopwords from NLTK, spaCy, and gensim, and returns a combined list of plain strings\n",
        "    \"\"\"\n",
        "    # load NLTK stopwords\n",
        "    nltk_stops = set(nltk_stopwords.words('english'))\n",
        "\n",
        "    # load spaCy model and extract stopwords\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "    spacy_stops = nlp.Defaults.stop_words\n",
        "\n",
        "    # combine with gensim stopwords\n",
        "    combined = nltk_stops.union(spacy_stops).union(gensim_stopwords)\n",
        "\n",
        "    # Return as a list of strings\n",
        "    return list(combined)\n",
        "\n",
        "\n",
        "\n",
        "# skip pre-1734 patents (before specifications were mandatory)\n",
        "dataset_full = load_dataset(\n",
        "    \"gbpatentdata/300YearsOfBritishPatents\",\n",
        "    data_files=\"texts.jsonl.gz\"\n",
        ")\n",
        "\n",
        "# skip pre-1734 patents (before specifications were mandatory)\n",
        "dataset_subset_range = dataset_full[\"train\"].filter(\n",
        "    lambda x: 1734 <= x[\"year\"] <= 1899\n",
        ")\n",
        "\n",
        "# keep only those with sufficient length\n",
        "dataset_subset_years = filter_dataset_by_length(dataset_subset_range[\"train\"], min_chars=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee88fc8-e817-47a7-bd7c-18b6c0f13eb8",
      "metadata": {
        "id": "1ee88fc8-e817-47a7-bd7c-18b6c0f13eb8"
      },
      "outputs": [],
      "source": [
        "### 1) Vectorizer: Build vocabulary + backward-IDF, then chunked transform\n",
        "\n",
        "class BackwardTFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    handles tf-bidf as in the KPST implementation. splits into fit() for vocab/bidf\n",
        "    and transform() that works in chunks for memory efficiency\n",
        "    \"\"\"\n",
        "    def __init__(self, min_df=5, max_df=0.5, stop_words=None, batch_size=1000):\n",
        "\n",
        "        # min number of docs a term must appear in\n",
        "        self.min_df = min_df\n",
        "\n",
        "        # max proportion of docs a term can appear in\n",
        "        self.max_df = max_df\n",
        "\n",
        "        # words to filter out\n",
        "        self.stop_words = set(stop_words) if stop_words else set()\n",
        "\n",
        "        # will store {term: index} mapping\n",
        "        self.vocabulary_ = {}\n",
        "\n",
        "        # how many docs to process at once\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def _compute_backward_idf_matrix(self, unique_years):\n",
        "        \"\"\"\n",
        "        calculates backwards idf scores - higher weight for rare terms before time t\n",
        "        bidf = log(total docs before t / (1 + docs with term before t))\n",
        "        \"\"\"\n",
        "        print(\"Computing backward-IDF values...\")\n",
        "\n",
        "        # order years and map them to matrix indices\n",
        "        sorted_years = np.sort(unique_years)\n",
        "        year_to_idx = {year: idx for idx, year in enumerate(sorted_years)}\n",
        "\n",
        "        # matrix to track term counts per year\n",
        "        n_terms = len(self.vocabulary_)\n",
        "        n_years = len(sorted_years)\n",
        "        term_year_counts = np.zeros((n_terms, n_years), dtype=np.float32)\n",
        "\n",
        "        print(f\"Creating matrix of shape: {term_year_counts.shape} for term-year counts\")\n",
        "\n",
        "        # fill the count matrix - how many docs per year have each term\n",
        "        for terms, year in tqdm(zip(self.doc_terms_, self.doc_years_),\n",
        "                                total=len(self.doc_terms_),\n",
        "                                desc=\"Counting term-year frequencies\"):\n",
        "\n",
        "            if year in year_to_idx:\n",
        "\n",
        "                yr_idx = year_to_idx[year]\n",
        "\n",
        "                for term in terms:\n",
        "\n",
        "                    if term in self.vocabulary_:\n",
        "\n",
        "                        term_idx = self.vocabulary_[term]\n",
        "\n",
        "                        term_year_counts[term_idx, yr_idx] += 1\n",
        "\n",
        "\n",
        "        print(\"Computing BIDF matrix...\")\n",
        "\n",
        "        # calculate backwards idf scores for each term-year pair\n",
        "        bidf_matrix = np.zeros_like(term_year_counts)\n",
        "\n",
        "        cumsum_terms = np.cumsum(term_year_counts, axis=1)  # cumulative sums across years\n",
        "\n",
        "        patents_prior = np.arange(1, n_years + 1, dtype=np.float32)  # docs up to each year\n",
        "\n",
        "        for i in range(n_years):\n",
        "\n",
        "            # bidf formula: log(total_docs_before / docs_with_term_before)\n",
        "            bidf_matrix[:, i] = np.log(\n",
        "                patents_prior[i] / (1 + cumsum_terms[:, i])\n",
        "            )\n",
        "\n",
        "        return bidf_matrix, year_to_idx\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        builds vocab dictionary, filters rare/common terms, makes bidf matrix.\n",
        "        needs years (y) to work!\n",
        "        \"\"\"\n",
        "        if y is None:\n",
        "\n",
        "            raise ValueError(\"Must provide 'y' as years to fit()!\")\n",
        "\n",
        "        print(\"Fitting BackwardTFIDFVectorizer...\")\n",
        "\n",
        "        # keep track of years and terms per doc\n",
        "        self.doc_years_ = y\n",
        "        self.doc_terms_ = []\n",
        "        term_counts = defaultdict(int)\n",
        "\n",
        "        # count how often each term appears\n",
        "        for doc in tqdm(X, desc=\"Parsing docs for vocabulary\"):\n",
        "            # get unique terms minus stopwords\n",
        "            tokens = set(t for t in doc.split() if t not in self.stop_words)\n",
        "\n",
        "            self.doc_terms_.append(tokens)\n",
        "\n",
        "            for t in tokens:\n",
        "                term_counts[t] += 1\n",
        "\n",
        "\n",
        "        # keep terms that aren't too rare or too common\n",
        "        n_docs = len(X)\n",
        "        min_docs = self.min_df if isinstance(self.min_df, int) else self.min_df * n_docs\n",
        "        max_docs = self.max_df if isinstance(self.max_df, int) else self.max_df * n_docs\n",
        "\n",
        "        filtered_terms = [\n",
        "            term for term, count in term_counts.items()\n",
        "            if min_docs <= count <= max_docs\n",
        "        ]\n",
        "\n",
        "        # create term->index mapping\n",
        "        self.vocabulary_ = {term: idx for idx, term in enumerate(filtered_terms)}\n",
        "\n",
        "        print(f\"Vocabulary size after filtering: {len(self.vocabulary_)}\")\n",
        "\n",
        "        # make the bidf matrix\n",
        "        unique_years = np.unique(y)\n",
        "        self.bidf_matrix_, self.year_to_idx_ = self._compute_backward_idf_matrix(unique_years)\n",
        "\n",
        "        # free up memory from the doc_terms_\n",
        "        del self.doc_terms_\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"\n",
        "        converts docs to tf-bidf vectors and normalizes them.\n",
        "        returns sparse matrix [n_docs x n_terms]\n",
        "        \"\"\"\n",
        "\n",
        "        if not hasattr(self, 'vocabulary_'):\n",
        "\n",
        "            raise ValueError(\"Must call fit() before transform()!\")\n",
        "\n",
        "        if y is None:\n",
        "\n",
        "            raise ValueError(\"Must provide years to transform()!\")\n",
        "\n",
        "\n",
        "        # prep for sparse matrix construction\n",
        "        rows, cols, data = [], [], []\n",
        "        n_docs = len(X)\n",
        "\n",
        "        # process docs in batches to save memory\n",
        "        for start_idx in range(0, n_docs, self.batch_size):\n",
        "\n",
        "            end_idx = min(start_idx + self.batch_size, n_docs)\n",
        "\n",
        "            batch_docs = X[start_idx:end_idx]\n",
        "\n",
        "            batch_years = y[start_idx:end_idx]\n",
        "\n",
        "            for local_idx, (doc, year) in enumerate(zip(batch_docs, batch_years)):\n",
        "\n",
        "                global_idx = start_idx + local_idx\n",
        "\n",
        "                # get term frequencies\n",
        "                terms = doc.split()\n",
        "\n",
        "                term_counter = Counter(t for t in terms if t in self.vocabulary_)\n",
        "\n",
        "                doc_len = sum(term_counter.values())\n",
        "\n",
        "                # if empty or year not in set, skip\n",
        "                if doc_len == 0 or (year not in self.year_to_idx_):\n",
        "                    continue\n",
        "                year_idx = self.year_to_idx_[year]\n",
        "\n",
        "                # build TF-BIDF for each term\n",
        "                for term, count in term_counter.items():\n",
        "\n",
        "                    term_idx = self.vocabulary_[term]\n",
        "\n",
        "                    tf = count / doc_len\n",
        "\n",
        "                    bidf = self.bidf_matrix_[term_idx, year_idx]\n",
        "\n",
        "                    rows.append(global_idx)\n",
        "\n",
        "                    cols.append(term_idx)\n",
        "\n",
        "                    data.append(tf * bidf)\n",
        "\n",
        "        # build the sparse matrix\n",
        "        mat = sparse.csr_matrix(\n",
        "            (data, (rows, cols)),\n",
        "            shape=(n_docs, len(self.vocabulary_))\n",
        "        )\n",
        "\n",
        "        # L2 normalize each row\n",
        "        row_norms = np.sqrt(mat.multiply(mat).sum(axis=1).A1)\n",
        "\n",
        "        row_norms[row_norms == 0] = 1e-9\n",
        "\n",
        "        mat = mat.multiply(1 / row_norms[:, np.newaxis])\n",
        "\n",
        "        return mat\n",
        "\n",
        "\n",
        "\n",
        "#### 2) Utility: Chunked transform -> store to disk\n",
        "\n",
        "def chunked_transform_to_disk(vectorizer, texts, years, chunk_size=5000, out_file=\"tfidf_all.npz\"):\n",
        "    \"\"\"\n",
        "    transform all documents in chunks, build one large CSR matrix in memory, then store to disk\n",
        "    \"\"\"\n",
        "    print(f\"\\nTransforming all docs in chunks of size {chunk_size}, then saving to disk: {out_file}\")\n",
        "\n",
        "\n",
        "    # transform each chunk\n",
        "    all_chunks = []\n",
        "\n",
        "    n_docs = len(texts)\n",
        "\n",
        "    for start in tqdm(range(0, n_docs, chunk_size), desc=\"Building TF-IDF for entire dataset\"):\n",
        "\n",
        "        end = min(start + chunk_size, n_docs)\n",
        "\n",
        "        mat_chunk = vectorizer.transform(texts[start:end], y=years[start:end])\n",
        "\n",
        "        all_chunks.append(mat_chunk)\n",
        "\n",
        "    # combine all chunk results\n",
        "    tfidf_all = vstack(all_chunks, format='csr')\n",
        "\n",
        "    sparse.save_npz(out_file, tfidf_all)\n",
        "\n",
        "    print(f\"TF-IDF matrix of shape {tfidf_all.shape} saved to {out_file}\")\n",
        "\n",
        "\n",
        "\n",
        "#### 3) Calculator Class: uses the disk-cached TF–IDF matrix\n",
        "\n",
        "class BreakthroughScoreCalculator:\n",
        "\n",
        "    \"\"\"\n",
        "    finds innovative patents by comparing similarity with past and future patents.\n",
        "    high future / low past similarity = potential breakthrough\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        stopwords,\n",
        "        forward_horizon=10,\n",
        "        backward_horizon=5,\n",
        "        min_df=0.0,\n",
        "        max_df=1.0,\n",
        "        similarity_threshold=0.05,\n",
        "        data_start_year=1734,\n",
        "        data_end_year=1899,\n",
        "        batch_size=1000,\n",
        "        n_jobs=-1\n",
        "    ):\n",
        "\n",
        "        # store all the settings\n",
        "        self.stopwords = stopwords\n",
        "        self.forward_horizon = forward_horizon\n",
        "        self.backward_horizon = backward_horizon\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.data_start_year = data_start_year\n",
        "        self.data_end_year = data_end_year\n",
        "        self.batch_size = batch_size\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "        # calculate valid year range\n",
        "        self.effective_start_year = self.data_start_year + self.backward_horizon\n",
        "\n",
        "        self.effective_end_year = self.data_end_year - self.forward_horizon\n",
        "\n",
        "        if self.effective_start_year >= self.effective_end_year:\n",
        "\n",
        "            raise ValueError(\"Invalid effective year range\")\n",
        "\n",
        "\n",
        "    # checks if we have enough past/future data for this year\n",
        "    def _is_within_effective_range(self, year):\n",
        "\n",
        "        return self.effective_start_year <= year <= self.effective_end_year\n",
        "\n",
        "\n",
        "\n",
        "    def _get_patent_text(self, patent_doc):\n",
        "        \"\"\"\n",
        "        combines all pages into one string and lowercases it\n",
        "        \"\"\"\n",
        "        pages = sorted(patent_doc['full_text'], key=lambda x: x['page_num'])\n",
        "\n",
        "        text = ' '.join(page['page_text'] for page in pages)\n",
        "\n",
        "        return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "    def compute_breakthrough_scores(self, dataset, tfidf_file=\"tfidf_all.npz\"):\n",
        "        \"\"\"\n",
        "        main pipeline:\n",
        "         1) Filter dataset into effective range\n",
        "         2) Fit BackwardTFIDFVectorizer\n",
        "         3) Transform entire subset (chunked), store to disk\n",
        "         4) Load stored matrix in memory\n",
        "         5) For each year, slice out current docs + forward/backward docs, compute similarities\n",
        "         6) Compute final metrics\n",
        "        \"\"\"\n",
        "        print(\"Filtering dataset by effective year range...\")\n",
        "\n",
        "        # get valid patents\n",
        "        patent_ids, years, texts = [], [], []\n",
        "\n",
        "        for pat in tqdm(dataset, desc=\"Reading dataset\"):\n",
        "\n",
        "            y = pat[\"year\"]\n",
        "\n",
        "            if self._is_within_effective_range(y):\n",
        "\n",
        "                patent_ids.append(pat[\"patent_id\"])\n",
        "\n",
        "                years.append(y)\n",
        "\n",
        "                texts.append(self._get_patent_text(pat))\n",
        "\n",
        "        if not texts:\n",
        "            raise ValueError(\"No patents in the effective range!\")\n",
        "\n",
        "\n",
        "\n",
        "        patent_ids = np.array(patent_ids)\n",
        "\n",
        "        years = np.array(years)\n",
        "\n",
        "        texts = np.array(texts, dtype=object)\n",
        "\n",
        "        # 1) fit vectorizer\n",
        "        print(f\"\\nFitting on {len(texts)} patents total...\")\n",
        "\n",
        "        self.vectorizer_ = BackwardTFIDFVectorizer(\n",
        "            min_df=self.min_df,\n",
        "            max_df=self.max_df,\n",
        "            stop_words=self.stopwords,\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        self.vectorizer_.fit(texts, y=years)\n",
        "\n",
        "        # 2) chunked transform all docs -> store to disk\n",
        "        chunked_transform_to_disk(self.vectorizer_, texts, years,\n",
        "                                  chunk_size=5000, out_file=tfidf_file)\n",
        "\n",
        "        # 3) load the full TF-IDF matrix\n",
        "        print(f\"\\nLoading TF-IDF matrix from disk: {tfidf_file}\")\n",
        "        tfidf_all = load_npz(tfidf_file)  # shape=(n_docs, vocab_size)\n",
        "\n",
        "        # 4) year-based index\n",
        "        year_to_indices = defaultdict(list)\n",
        "\n",
        "        for i, y in enumerate(years):\n",
        "\n",
        "            year_to_indices[y].append(i)\n",
        "\n",
        "        # prepare arrays for sums and counts\n",
        "        n = len(texts)\n",
        "        fwd_sums = np.zeros(n, dtype=np.float64)\n",
        "        fwd_counts = np.zeros(n, dtype=np.float64)\n",
        "        bwd_sums = np.zeros(n, dtype=np.float64)\n",
        "        bwd_counts = np.zeros(n, dtype=np.float64)\n",
        "\n",
        "        # 5) for each year, slice row subsets\n",
        "        unique_effective_years = sorted(year_to_indices.keys())\n",
        "\n",
        "        for y in tqdm(unique_effective_years, desc=\"Year-based loop\"):\n",
        "\n",
        "            indices_current_year = year_to_indices[y]\n",
        "\n",
        "            if not indices_current_year:\n",
        "                continue\n",
        "\n",
        "            current_matrix = tfidf_all[indices_current_year]\n",
        "\n",
        "            # forward horizon\n",
        "            fwd_indices = []\n",
        "\n",
        "            for fwd_yr in range(y+1, y+1+self.forward_horizon):\n",
        "\n",
        "                if fwd_yr in year_to_indices:\n",
        "\n",
        "                    fwd_indices.extend(year_to_indices[fwd_yr])\n",
        "\n",
        "            if fwd_indices:\n",
        "\n",
        "                fwd_matrix = tfidf_all[fwd_indices]\n",
        "\n",
        "                sim_fwd = current_matrix.dot(fwd_matrix.T).tocsr()\n",
        "\n",
        "                # threshold\n",
        "                sim_fwd.data[sim_fwd.data < self.similarity_threshold] = 0\n",
        "\n",
        "                sim_fwd.eliminate_zeros()\n",
        "\n",
        "                # sums, counts\n",
        "                sums_local = np.array(sim_fwd.sum(axis=1)).ravel()\n",
        "\n",
        "                counts_local = np.diff(sim_fwd.indptr)\n",
        "\n",
        "                fwd_sums[indices_current_year] = sums_local\n",
        "\n",
        "                fwd_counts[indices_current_year] = counts_local\n",
        "\n",
        "            # backward horizon\n",
        "            bwd_indices = []\n",
        "\n",
        "            for bwd_yr in range(y - self.backward_horizon, y):\n",
        "\n",
        "                if bwd_yr in year_to_indices:\n",
        "\n",
        "                    bwd_indices.extend(year_to_indices[bwd_yr])\n",
        "\n",
        "            if bwd_indices:\n",
        "\n",
        "                bwd_matrix = tfidf_all[bwd_indices]\n",
        "\n",
        "                sim_bwd = current_matrix.dot(bwd_matrix.T).tocsr()\n",
        "\n",
        "                sim_bwd.data[sim_bwd.data < self.similarity_threshold] = 0\n",
        "\n",
        "                sim_bwd.eliminate_zeros()\n",
        "\n",
        "                sums_local = np.array(sim_bwd.sum(axis=1)).ravel()\n",
        "\n",
        "                counts_local = np.diff(sim_bwd.indptr)\n",
        "\n",
        "                bwd_sums[indices_current_year] = sums_local\n",
        "\n",
        "                bwd_counts[indices_current_year] = counts_local\n",
        "\n",
        "        # 6) final breakthrough metrics\n",
        "        print(\"Computing final breakthrough metrics...\")\n",
        "\n",
        "        fwd_avgs = np.divide(\n",
        "            fwd_sums, fwd_counts,\n",
        "            out=np.zeros_like(fwd_sums), where=(fwd_counts > 0)\n",
        "        )\n",
        "\n",
        "        bwd_avgs = np.divide(\n",
        "            bwd_sums, bwd_counts,\n",
        "            out=np.zeros_like(bwd_sums), where=(bwd_counts > 0)\n",
        "        )\n",
        "\n",
        "        ############# NOTE: this section is different from KPST ###################\n",
        "\n",
        "\n",
        "        bwd_avgs_safe = np.where(bwd_avgs == 0, 1e-9, bwd_avgs)\n",
        "\n",
        "        # here the breakthrough score is an average, so centered on 1\n",
        "        breakthrough_scores = fwd_avgs / bwd_avgs_safe\n",
        "\n",
        "\n",
        "        ##### The KSPT implementation is to SUM over the fwd and bwd scores\n",
        "        ##### To implement the exact KPST breakthrough scores, replace the two lines above with the two lines below:\n",
        "\n",
        "        # bwd_sums_safe = np.where(bwd_sums == 0, 1e-9, bwd_sums)\n",
        "        # breakthrough_scores = fwd_sums / bwd_sums_safe\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        # build df\n",
        "        results_df = pd.DataFrame({\n",
        "            \"patent_id\": patent_ids,\n",
        "            \"year\": years,\n",
        "            \"forward_similarity\": fwd_avgs,\n",
        "            \"backward_similarity\": bwd_avgs,\n",
        "            \"breakthrough_score\": breakthrough_scores\n",
        "        })\n",
        "        return results_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(dataset,\n",
        "         forward_horizon=10,\n",
        "         backward_horizon=5,\n",
        "         min_df=0.0,\n",
        "         max_df=1.0,\n",
        "         similarity_threshold=0.05,\n",
        "         data_start_year=1734,\n",
        "         data_end_year=1899,\n",
        "         batch_size=1000,\n",
        "         n_jobs=-1,\n",
        "         tfidf_file=\"tfidf_all.npz\"):\n",
        "\n",
        "    # define stopwords\n",
        "    stopwords = load_all_stopwords()\n",
        "\n",
        "    # instantiate breakthrough_scores calculator\n",
        "    calculator = BreakthroughScoreCalculator(\n",
        "        stopwords=stopwords,\n",
        "        forward_horizon=forward_horizon,\n",
        "        backward_horizon=backward_horizon,\n",
        "        min_df=min_df,\n",
        "        max_df=max_df,\n",
        "        similarity_threshold=similarity_threshold,\n",
        "        data_start_year=data_start_year,\n",
        "        data_end_year=data_end_year,\n",
        "        batch_size=batch_size,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "    # compute breakthroughs\n",
        "    results_df = calculator.compute_breakthrough_scores(\n",
        "        dataset, tfidf_file=tfidf_file\n",
        "    )\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e981c0-4444-455f-a945-988138eba10a",
      "metadata": {
        "scrolled": true,
        "id": "01e981c0-4444-455f-a945-988138eba10a",
        "outputId": "a9b750f0-78d0-4f1e-b31c-cda9c553736b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering dataset by effective year range...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading dataset:  14%|█▍        | 43895/318472 [01:14<02:45, 1654.12it/s]"
          ]
        }
      ],
      "source": [
        "# input list of patent dicts called `dataset_subset_years` from earlier load_dataset:\n",
        "results = main(\n",
        "    dataset=dataset_subset_years,\n",
        "    forward_horizon=10,\n",
        "    backward_horizon=5,\n",
        "    min_df=0.0,\n",
        "    max_df=1.0,\n",
        "    similarity_threshold=0.05,\n",
        "    batch_size=5000,\n",
        "    n_jobs=-1,\n",
        "    tfidf_file=\"my_tfidf_data_fh10_bh5_maxdf100.npz\"\n",
        ")\n",
        "results.to_csv(\"breakthrough_scores_fh10_bh5_maxdf100.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8724a422-88e4-4f81-abce-52b387a4928e",
      "metadata": {
        "id": "8724a422-88e4-4f81-abce-52b387a4928e"
      },
      "outputs": [],
      "source": [
        "results = main(\n",
        "    dataset=dataset_subset_years,\n",
        "    forward_horizon=5,\n",
        "    backward_horizon=5,\n",
        "    min_df=0.0,\n",
        "    max_df=1.0,\n",
        "    similarity_threshold=0.05,\n",
        "    batch_size=5000,\n",
        "    n_jobs=-1,\n",
        "    tfidf_file=\"my_tfidf_data_fh5_bh5_maxdf100.npz\"\n",
        ")\n",
        "results.to_csv(\"breakthrough_scores_fh5_bh5_maxdf100.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9aab54-3021-4468-b27c-e2b1d9eb81ba",
      "metadata": {
        "id": "ae9aab54-3021-4468-b27c-e2b1d9eb81ba"
      },
      "outputs": [],
      "source": [
        "results = main(\n",
        "    dataset=dataset_subset_years,\n",
        "    forward_horizon=1,\n",
        "    backward_horizon=5,\n",
        "    min_df=0.0,\n",
        "    max_df=1.0,\n",
        "    similarity_threshold=0.05,\n",
        "    batch_size=5000,\n",
        "    n_jobs=-1,\n",
        "    tfidf_file=\"my_tfidf_data_fh1_bh5_maxdf100.npz\"\n",
        ")\n",
        "results.to_csv(\"breakthrough_scores_fh1_bh5_maxdf100.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ea954b-a7a8-436f-be3e-36640beda8cf",
      "metadata": {
        "id": "38ea954b-a7a8-436f-be3e-36640beda8cf"
      },
      "outputs": [],
      "source": [
        "results = main(\n",
        "    dataset=dataset_subset_years,\n",
        "    forward_horizon=20,\n",
        "    backward_horizon=5,\n",
        "    min_df=0.0,\n",
        "    max_df=1.0,\n",
        "    similarity_threshold=0.05,\n",
        "    batch_size=5000,\n",
        "    n_jobs=-1,\n",
        "    tfidf_file=\"my_tfidf_data_fh20_bh5_maxdf100.npz\"\n",
        ")\n",
        "results.to_csv(\"breakthrough_scores_fh20_bh5_maxdf100.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}