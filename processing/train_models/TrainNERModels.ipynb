{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543722e-9830-4dee-af37-4fc8af9df5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers seqeval[gpu]\n",
    "#!pip install torch\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9188948-8a5a-47d5-89b9-ee36542c60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "import wandb  \n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437c50bc-0005-42d4-a8af-780620277614",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESS DATA ### \n",
    "\n",
    "def preprocess(file_path: str):\n",
    "   \"\"\"\n",
    "   Read and process a CoNLL format file into a structured DataFrame with sentence-level annotations.\n",
    "   \n",
    "   The function processes files where each line contains a word and its tag, sentences are separated\n",
    "   by blank lines, and returns processed data with sentence-level aggregation.\n",
    "   \n",
    "   Args:\n",
    "       file_path (str): Path to the CoNLL format file to be processed\n",
    "       \n",
    "   Returns:\n",
    "       tuple: Contains:\n",
    "           - DataFrame: Processed data with columns ['sentence', 'word_labels']\n",
    "           - dict: Mapping of labels to unique IDs (label2id)\n",
    "           - dict: Mapping of IDs back to labels (id2label)\n",
    "           \n",
    "   Note:\n",
    "       - Words in sentences are joined with spaces\n",
    "       - Tags are joined with commas\n",
    "       - Empty values are replaced with 'NaN'\n",
    "   \"\"\"\n",
    "   # initialize lists to store sentence data\n",
    "   sentences = []\n",
    "   sentence_no = 1\n",
    "   sentence_words = []\n",
    "   sentence_tags = []\n",
    "   \n",
    "   # process file line by line\n",
    "   with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "       for line in f:\n",
    "           line = line.strip()\n",
    "           \n",
    "           # handle sentence boundaries\n",
    "           if not line:\n",
    "               if sentence_words and sentence_tags:\n",
    "                   sentences.append((sentence_no, sentence_words, sentence_tags))\n",
    "               sentence_no += 1\n",
    "               sentence_words = []\n",
    "               sentence_tags = []\n",
    "           \n",
    "           # extract word and tag from line\n",
    "           else:\n",
    "               word, tag = line.split()[0], line.split()[-1]\n",
    "               sentence_words.append(word or \"NaN\")\n",
    "               sentence_tags.append(tag or \"NaN\")\n",
    "   \n",
    "   # add final sentence if exists\n",
    "   if sentence_words and sentence_tags:\n",
    "       sentences.append((sentence_no, sentence_words, sentence_tags))\n",
    "   \n",
    "   # create dataframe from processed sentences\n",
    "   data = pd.DataFrame({\n",
    "       \"sentence_no\": [s[0] for s in sentences],\n",
    "       \"word\": [\" \".join(s[1]) for s in sentences],\n",
    "       \"tag\": [\",\".join(s[2]) for s in sentences]\n",
    "   })\n",
    "   \n",
    "   # create label mappings\n",
    "   unique_tags = sorted(set(tag for tags in data[\"tag\"] for tag in tags.split(\",\")))\n",
    "   label2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "   id2label = {idx: tag for tag, idx in label2id.items()}\n",
    "   \n",
    "   # prepare final dataframe\n",
    "   data = data[[\"word\", \"tag\"]].drop_duplicates().reset_index(drop=True)\n",
    "   data.columns = [\"sentence\", \"word_labels\"]\n",
    "   \n",
    "   return data, label2id, id2label\n",
    "\n",
    "\n",
    "data, label2id, id2label = preprocess('data_title.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea1f8c2-2856-40a1-88aa-e821e4a06e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMS ### \n",
    "\n",
    "default_parameters = {\n",
    "    'MAX_LEN': 512,\n",
    "    'TRAIN_BATCH_SIZE': 21, \n",
    "    'VALID_BATCH_SIZE': 21, \n",
    "    'EPOCHS': 30, \n",
    "    'MAX_GRAD_NORM': 10,\n",
    "    'model_name': 'xlm-roberta-large',  # Default model\n",
    "    'tokenizer_name': 'xlm-roberta-large'  # Default tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584ca4f8-8254-4636-b770-13151afa197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/example-project2/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### INITIALIZE TOKENIZER ### \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(default_parameters['tokenizer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83eca30c-ef19-4695-aa71-05a01074cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE WORD TOKENIZER ### \n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function and description (below) modified from https://github.com/chambliss/Multilingual_NER/blob/8d3afffd4c99774e0585f4c7d721bb99481fd60f/python/utils/main_utils.py#L118\n",
    "    \n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence.strip().split(), text_labels.split(\",\")):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * len(tokenized_word))\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ce2625-d64c-496c-9931-384ccd62234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET CLASS ### \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, label2id):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        # Tokenize the sentence with offsets\n",
    "        encoding = self.tokenizer(\n",
    "            sentence.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        labels = [self.label2id[label] for label in word_labels]\n",
    "        word_ids = encoding.word_ids()\n",
    "\n",
    "        # Create label_ids aligned with tokens\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignored when computing loss\n",
    "            else:\n",
    "                label_ids.append(labels[word_idx])\n",
    "\n",
    "        # Prepare item\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(label_ids)\n",
    "        item['texts'] = sentence  # Include original text for prediction\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81fe142-d8dd-4be9-b90f-44d0d6d0de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (401, 2)\n",
      "TRAIN Dataset: (240, 2)\n",
      "VALIDATION Dataset: (80, 2)\n",
      "TEST Dataset: (81, 2)\n"
     ]
    }
   ],
   "source": [
    "### CREATE DATA SPLITS ### \n",
    "\n",
    "train_size = 0.6\n",
    "val_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=(1 - train_size), random_state=1234)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=(test_size / (val_size + test_size)), random_state=1234)\n",
    "\n",
    "print(f\"FULL Dataset: {data.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_data.shape}\")\n",
    "print(f\"VALIDATION Dataset: {val_data.shape}\")\n",
    "print(f\"TEST Dataset: {test_data.shape}\")\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d97944d-a431-42fa-a532-b274256bb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_titles.csv\") \n",
    "val_data.to_csv(\"val_titles.csv\") \n",
    "test_data.to_csv(\"test_titles.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51083de-6360-48fc-8d51-01db1234cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated = {key: [] for key in keys}\n",
    "\n",
    "    for item in batch:\n",
    "        for key in keys:\n",
    "            collated[key].append(item[key])\n",
    "\n",
    "    # Stack tensors for input_ids, attention_mask, labels\n",
    "    for key in ['input_ids', 'attention_mask', 'labels']:\n",
    "        collated[key] = torch.stack(collated[key])\n",
    "\n",
    "    # Pad offset_mapping\n",
    "    max_len = max([len(o) for o in collated['offset_mapping']])\n",
    "    padded_offsets = []\n",
    "    for offsets in collated['offset_mapping']:\n",
    "        if len(offsets) < max_len:\n",
    "            padding = torch.zeros((max_len - len(offsets), 2), dtype=torch.long)\n",
    "            offsets = torch.cat((offsets, padding), dim=0)\n",
    "        padded_offsets.append(offsets)\n",
    "    collated['offset_mapping'] = torch.stack(padded_offsets)\n",
    "\n",
    "    # 'texts' is already a list of strings, no need to process further\n",
    "    # Ensure 'texts' is included in collated\n",
    "    # collated['texts'] = collated['texts']  # This line is unnecessary\n",
    "\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c182e11-8eda-4982-a2ff-1effeb46e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUILD DATALOADERS ### \n",
    "\n",
    "training_set = CustomDataset(train_data, tokenizer, default_parameters['MAX_LEN'], label2id)\n",
    "validation_set = CustomDataset(val_data, tokenizer, default_parameters['MAX_LEN'], label2id)\n",
    "testing_set = CustomDataset(test_data, tokenizer, default_parameters['MAX_LEN'], label2id)\n",
    "\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=default_parameters['TRAIN_BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=default_parameters['VALID_BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "testing_loader = DataLoader(\n",
    "    testing_set,\n",
    "    batch_size=default_parameters['VALID_BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59af7109-1a9e-4e53-99be-fa9565970042",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZE MODEL ### \n",
    "\n",
    "def prepare_model(config):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af30a1f9-3be6-4437-8905-62892a2d0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTRUCT FULL NAMED ENTITIES FROM TOKENS ### \n",
    "\n",
    "def reconstruct_entities(tokens, labels, offsets):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for idx, (token, label, offset) in enumerate(zip(tokens, labels, offsets)):\n",
    "        start, end = offset\n",
    "\n",
    "        # Skip tokens with no offset (special tokens, padding)\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        # Replace special characters if necessary (adjust for your tokenizer)\n",
    "        word = token.replace('▁', ' ')\n",
    "\n",
    "        # Aggregate I-O-B tagged entities\n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {'type': label[2:], 'text': word.strip(), 'start': start, 'end': end}\n",
    "        elif label.startswith('I-'):\n",
    "            if current_entity and label[2:] == current_entity['type']:\n",
    "                current_entity['text'] += word\n",
    "                current_entity['end'] = end\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = {'type': label[2:], 'text': word.strip(), 'start': start, 'end': end}\n",
    "        else:\n",
    "            # Deal with 'O' tag\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    # Merge entities of the same type if they are adjacent\n",
    "    merged_entities = []\n",
    "    for entity in entities:\n",
    "        if merged_entities and merged_entities[-1]['type'] == entity['type'] and merged_entities[-1]['end'] == entity['start']:\n",
    "            merged_entities[-1]['text'] += entity['text']\n",
    "            merged_entities[-1]['end'] = entity['end']\n",
    "        else:\n",
    "            merged_entities.append(entity)\n",
    "\n",
    "    # Clean up extra spaces\n",
    "    for entity in merged_entities:\n",
    "        entity['text'] = ' '.join(entity['text'].split())\n",
    "        entity['start'] = int(entity['start'])\n",
    "        entity['end'] = int(entity['end'])\n",
    "\n",
    "    return merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd41d430-4521-46ae-b0da-9edf3a0a327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### F1 METRICS ### \n",
    "\n",
    "def compute_f1_scores(true_entities_list, pred_entities_list):\n",
    "   \"\"\"\n",
    "   Compute precision, recall, and F1 scores for named entity recognition.\n",
    "   \n",
    "   Args:\n",
    "       true_entities_list (list): Ground truth entities\n",
    "       pred_entities_list (list): Predicted entities\n",
    "       \n",
    "   Returns:\n",
    "       dict: F1 scores including per-class, micro, macro and weighted averages\n",
    "   \"\"\"\n",
    "   # initialize metric counters\n",
    "   class_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "   total_metrics = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "   total_support = 0\n",
    "   \n",
    "   # compute true/false positives/negatives\n",
    "   for ground_truth, predictions in zip(true_entities_list, pred_entities_list):\n",
    "       ground_truth_set = set((e['type'], e['text']) for e in ground_truth)\n",
    "       predictions_set = set((e['type'], e['text']) for e in predictions)\n",
    "       \n",
    "       # count true positives and false negatives\n",
    "       for entity in ground_truth_set:\n",
    "           if entity in predictions_set:\n",
    "               class_metrics[entity[0]][\"tp\"] += 1\n",
    "               total_metrics[\"tp\"] += 1\n",
    "           else:\n",
    "               class_metrics[entity[0]][\"fn\"] += 1\n",
    "               total_metrics[\"fn\"] += 1\n",
    "       \n",
    "       # count false positives\n",
    "       for entity in predictions_set:\n",
    "           if entity not in ground_truth_set:\n",
    "               class_metrics[entity[0]][\"fp\"] += 1\n",
    "               total_metrics[\"fp\"] += 1\n",
    "   \n",
    "   # compute per-class metrics\n",
    "   f1_scores = {}\n",
    "   total_precision = 0\n",
    "   total_recall = 0\n",
    "   total_f1 = 0\n",
    "   \n",
    "   for entity_class, metrics in class_metrics.items():\n",
    "       tp = metrics[\"tp\"]\n",
    "       fp = metrics[\"fp\"]\n",
    "       fn = metrics[\"fn\"]\n",
    "       support = tp + fn\n",
    "       \n",
    "       # calculate precision, recall, f1\n",
    "       precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "       recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "       f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "       \n",
    "       f1_scores[entity_class] = {\n",
    "           \"precision\": precision,\n",
    "           \"recall\": recall,\n",
    "           \"f1-score\": f1,\n",
    "           \"support\": support\n",
    "       }\n",
    "       \n",
    "       total_precision += precision\n",
    "       total_recall += recall\n",
    "       total_f1 += f1\n",
    "       total_support += support\n",
    "   \n",
    "   # compute macro averages\n",
    "   num_classes = len(class_metrics)\n",
    "   macro_precision = total_precision / num_classes if num_classes > 0 else 0\n",
    "   macro_recall = total_recall / num_classes if num_classes > 0 else 0\n",
    "   macro_f1 = total_f1 / num_classes if num_classes > 0 else 0\n",
    "   \n",
    "   # compute micro averages\n",
    "   tp_total = total_metrics[\"tp\"]\n",
    "   fp_total = total_metrics[\"fp\"]\n",
    "   fn_total = total_metrics[\"fn\"]\n",
    "   micro_precision = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "   micro_recall = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "   micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "   \n",
    "   # compute weighted averages\n",
    "   weighted_precision = sum((metrics[\"precision\"] * metrics[\"support\"]) for metrics in f1_scores.values()) / total_support if total_support > 0 else 0\n",
    "   weighted_recall = sum((metrics[\"recall\"] * metrics[\"support\"]) for metrics in f1_scores.values()) / total_support if total_support > 0 else 0\n",
    "   weighted_f1 = sum((metrics[\"f1-score\"] * metrics[\"support\"]) for metrics in f1_scores.values()) / total_support if total_support > 0 else 0\n",
    "   \n",
    "   # add average metrics to results\n",
    "   f1_scores.update({\n",
    "       \"micro avg\": {\n",
    "           \"precision\": micro_precision,\n",
    "           \"recall\": micro_recall,\n",
    "           \"f1-score\": micro_f1,\n",
    "           \"support\": total_support\n",
    "       },\n",
    "       \"macro avg\": {\n",
    "           \"precision\": macro_precision,\n",
    "           \"recall\": macro_recall,\n",
    "           \"f1-score\": macro_f1,\n",
    "           \"support\": total_support\n",
    "       },\n",
    "       \"weighted avg\": {\n",
    "           \"precision\": weighted_precision,\n",
    "           \"recall\": weighted_recall,\n",
    "           \"f1-score\": weighted_f1,\n",
    "           \"support\": total_support\n",
    "       }\n",
    "   })\n",
    "   \n",
    "   return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4e2111b-4fe8-4472-99a3-c77d996996e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN FOR A SINGLE EPOCH ### \n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, epoch, config):\n",
    "   \"\"\"\n",
    "   Train model for one epoch.\n",
    "   \n",
    "   Args:\n",
    "       model: NER model\n",
    "       optimizer: Optimizer instance\n",
    "       train_loader: Training data loader\n",
    "       epoch (int): Current epoch number \n",
    "       config (dict): Training configuration parameters\n",
    "   \"\"\"\n",
    "   # set training mode\n",
    "   model.train()\n",
    "   total_loss = 0\n",
    "   nb_tr_steps = 0\n",
    "   \n",
    "   # train on batches\n",
    "   for idx, batch in enumerate(train_loader):\n",
    "       # move batch to device\n",
    "       ids = batch['input_ids'].to(device)\n",
    "       mask = batch['attention_mask'].to(device)\n",
    "       targets = batch['labels'].to(device)\n",
    "       \n",
    "       # forward pass\n",
    "       outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "       loss = outputs.loss\n",
    "       \n",
    "       # accumulate loss\n",
    "       total_loss += loss.item()\n",
    "       nb_tr_steps += 1\n",
    "       \n",
    "       # backward pass with gradient clipping\n",
    "       torch.nn.utils.clip_grad_norm_(\n",
    "           parameters=model.parameters(), \n",
    "           max_norm=config['MAX_GRAD_NORM']\n",
    "       )\n",
    "       optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "   \n",
    "   # compute average loss and log\n",
    "   avg_epoch_loss = total_loss / nb_tr_steps\n",
    "   wandb.log({'train_loss': avg_epoch_loss, 'epoch': epoch + 1})\n",
    "   print(f\"Epoch {epoch + 1}/{config['EPOCHS']}, Training Loss: {avg_epoch_loss}\")\n",
    "\n",
    "\n",
    "def validate(model, val_loader, epoch, config):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    true_entities_list = []\n",
    "    pred_entities_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            offsets = batch['offset_mapping']\n",
    "            texts = batch['texts']  # 'texts' should now be included\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            for i in range(input_ids.size(0)):\n",
    "                # Move tensors to CPU\n",
    "                input_ids_i = input_ids[i].cpu()\n",
    "                pred_labels_i = preds[i].cpu()\n",
    "                true_labels_i = labels[i].cpu()\n",
    "                attention_mask_i = attention_mask[i].cpu()\n",
    "                offset_mapping_i = offsets[i]\n",
    "                text = texts[i]\n",
    "\n",
    "                active_tokens = attention_mask_i == 1\n",
    "                input_ids_i = input_ids_i[active_tokens].numpy()\n",
    "                pred_labels_i = pred_labels_i[active_tokens].numpy()\n",
    "                true_labels_i = true_labels_i[active_tokens].numpy()\n",
    "                offset_mapping_i = offset_mapping_i[active_tokens].numpy()\n",
    "\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids_i)\n",
    "                pred_tags = [id2label[label_id] if label_id != -100 else 'O' for label_id in pred_labels_i]\n",
    "                true_tags = [id2label[label_id] if label_id != -100 else 'O' for label_id in true_labels_i]\n",
    "\n",
    "                pred_entities = reconstruct_entities(tokens, pred_tags, offset_mapping_i)\n",
    "                true_entities = reconstruct_entities(tokens, true_tags, offset_mapping_i)\n",
    "\n",
    "                pred_entities_list.append(pred_entities)\n",
    "                true_entities_list.append(true_entities)\n",
    "\n",
    "    avg_eval_loss = eval_loss / nb_eval_steps\n",
    "    f1_scores = compute_f1_scores(true_entities_list, pred_entities_list)\n",
    "    micro_f1 = f1_scores['micro avg']['f1-score']\n",
    "    wandb.log({'validation_loss': avg_eval_loss, 'validation_f1': micro_f1, 'epoch': epoch + 1})\n",
    "    print(f\"Validation Loss: {avg_eval_loss}\")\n",
    "    print(f\"Validation F1 Score (Entity Level): {micro_f1}\")\n",
    "\n",
    "    return avg_eval_loss, micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88a2947-33f6-484f-982f-a00edb413232",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN ### \n",
    "\n",
    "def train(config=None):\n",
    "   \"\"\"\n",
    "   Train NER model with given configuration.\n",
    "   \n",
    "   Args:\n",
    "       config (dict, optional): Training configuration parameters.\n",
    "           If None, uses default_parameters\n",
    "           \n",
    "   Returns:\n",
    "       tuple: Best validation F1 score and learning rate used\n",
    "   \"\"\"\n",
    "   # setup configuration\n",
    "   if config is None:\n",
    "       config = default_parameters\n",
    "   else:\n",
    "       config = dict(config)\n",
    "       config.update(default_parameters)\n",
    "       \n",
    "   # initialize model and optimizer\n",
    "   model = prepare_model(config)\n",
    "   optimizer = torch.optim.Adam(\n",
    "       params=model.parameters(), \n",
    "       lr=config['learning_rate']\n",
    "   )\n",
    "   \n",
    "   # track best model\n",
    "   best_f1 = 0.0\n",
    "   best_epoch = 0\n",
    "   best_model_state = None\n",
    "   \n",
    "   # training loop\n",
    "   for epoch in range(config['EPOCHS']):\n",
    "       print(f\"Training epoch: {epoch + 1}/{config['EPOCHS']}\")\n",
    "       \n",
    "       # train and validate\n",
    "       train_epoch(model, optimizer, training_loader, epoch, config)\n",
    "       print(\"Running validation...\")\n",
    "       val_loss, val_f1 = validate(model, validation_loader, epoch, config)\n",
    "       \n",
    "       # save best model\n",
    "       if val_f1 > best_f1:\n",
    "           best_f1 = val_f1\n",
    "           best_epoch = epoch + 1\n",
    "           best_model_state = model.state_dict()\n",
    "           \n",
    "           # save model with learning rate in filename\n",
    "           lr_str = f\"{config['learning_rate']:.10e}\"\n",
    "           model_filename = f'best_model_lr_{lr_str}.pt'\n",
    "           torch.save(best_model_state, model_filename)\n",
    "           print(f\"Best model saved at epoch {best_epoch} with F1 score: {best_f1} as {model_filename}\")\n",
    "   \n",
    "   # log final results\n",
    "   print(f\"Training completed. Best validation F1 score: {best_f1} at epoch {best_epoch}\")\n",
    "   wandb.log({\n",
    "       'best_epoch': best_epoch, \n",
    "       'best_validation_f1': best_f1\n",
    "   })\n",
    "   \n",
    "   return best_f1, config['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c5959a-ffa0-4a87-93b7-751cbdfa92a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bksuaru8\n",
      "Sweep URL: https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/bksuaru8\n"
     ]
    }
   ],
   "source": [
    "# W&B SWEEP CONFIGS \n",
    "sweep_config = {\n",
    "    'method': 'grid',  # Change to grid search\n",
    "    'metric': {\n",
    "        'name': 'best_validation_f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [2e-05, 3e-05, 4e-05, 5e-05, 6e-05]  # Specify exact values to test\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# INITIALIZE SWEEP \n",
    "sweep_id = wandb.sweep(sweep_config, project=\"gbpatentdata_grid_xlm_title\")\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init():\n",
    "        config = wandb.config\n",
    "        # Train the model with the current hyperparameters\n",
    "        best_f1, lr = train(config)\n",
    "\n",
    "        # Log the best validation F1 score and learning rate\n",
    "        wandb.log({'best_validation_f1': best_f1, 'learning_rate': lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "929d2eeb-6d1a-47ea-944a-5c21e9636f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ukrvwiw8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthewleechen\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bizon/example-project2/science_technology/gbpatentdata/wandb/run-20241111_113134-ukrvwiw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/ukrvwiw8' target=\"_blank\">deft-sweep-1</a></strong> to <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/ukrvwiw8' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/ukrvwiw8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "Epoch 1/30, Training Loss: 0.6016708438595136\n",
      "Running validation...\n",
      "Validation Loss: 0.21204044669866562\n",
      "Validation F1 Score (Entity Level): 0\n",
      "Training epoch: 2/30\n",
      "Epoch 2/30, Training Loss: 0.16986056044697762\n",
      "Running validation...\n",
      "Validation Loss: 0.12579835206270218\n",
      "Validation F1 Score (Entity Level): 0\n",
      "Training epoch: 3/30\n",
      "Epoch 3/30, Training Loss: 0.09591686104734738\n",
      "Running validation...\n",
      "Validation Loss: 0.05564243160188198\n",
      "Validation F1 Score (Entity Level): 0.3404255319148936\n",
      "Best model saved at epoch 3 with F1 score: 0.3404255319148936 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 4/30\n",
      "Epoch 4/30, Training Loss: 0.04182384628802538\n",
      "Running validation...\n",
      "Validation Loss: 0.02374283759854734\n",
      "Validation F1 Score (Entity Level): 0.5434782608695653\n",
      "Best model saved at epoch 4 with F1 score: 0.5434782608695653 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 5/30\n",
      "Epoch 5/30, Training Loss: 0.017668565657610696\n",
      "Running validation...\n",
      "Validation Loss: 0.011719555594027042\n",
      "Validation F1 Score (Entity Level): 0.5464480874316939\n",
      "Best model saved at epoch 5 with F1 score: 0.5464480874316939 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 6/30\n",
      "Epoch 6/30, Training Loss: 0.0086246602392445\n",
      "Running validation...\n",
      "Validation Loss: 0.007750970660708845\n",
      "Validation F1 Score (Entity Level): 0.5945945945945946\n",
      "Best model saved at epoch 6 with F1 score: 0.5945945945945946 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 7/30\n",
      "Epoch 7/30, Training Loss: 0.005937701789662242\n",
      "Running validation...\n",
      "Validation Loss: 0.0066551094641909\n",
      "Validation F1 Score (Entity Level): 0.6373626373626373\n",
      "Best model saved at epoch 7 with F1 score: 0.6373626373626373 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 8/30\n",
      "Epoch 8/30, Training Loss: 0.00475902168545872\n",
      "Running validation...\n",
      "Validation Loss: 0.00569716258905828\n",
      "Validation F1 Score (Entity Level): 0.6629834254143646\n",
      "Best model saved at epoch 8 with F1 score: 0.6629834254143646 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 9/30\n",
      "Epoch 9/30, Training Loss: 0.0035109060214987644\n",
      "Running validation...\n",
      "Validation Loss: 0.00521899905288592\n",
      "Validation F1 Score (Entity Level): 0.6519337016574586\n",
      "Training epoch: 10/30\n",
      "Epoch 10/30, Training Loss: 0.0025116519536823034\n",
      "Running validation...\n",
      "Validation Loss: 0.00425217259908095\n",
      "Validation F1 Score (Entity Level): 0.6850828729281768\n",
      "Best model saved at epoch 10 with F1 score: 0.6850828729281768 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 11/30\n",
      "Epoch 11/30, Training Loss: 0.002304513929023718\n",
      "Running validation...\n",
      "Validation Loss: 0.0067579447641037405\n",
      "Validation F1 Score (Entity Level): 0.6553672316384181\n",
      "Training epoch: 12/30\n",
      "Epoch 12/30, Training Loss: 0.002043406537268311\n",
      "Running validation...\n",
      "Validation Loss: 0.0030655600130558014\n",
      "Validation F1 Score (Entity Level): 0.735632183908046\n",
      "Best model saved at epoch 12 with F1 score: 0.735632183908046 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 13/30\n",
      "Epoch 13/30, Training Loss: 0.0016728077656201397\n",
      "Running validation...\n",
      "Validation Loss: 0.003444713191129267\n",
      "Validation F1 Score (Entity Level): 0.6927374301675978\n",
      "Training epoch: 14/30\n",
      "Epoch 14/30, Training Loss: 0.0016085909398195024\n",
      "Running validation...\n",
      "Validation Loss: 0.0029171962523832917\n",
      "Validation F1 Score (Entity Level): 0.6966292134831461\n",
      "Training epoch: 15/30\n",
      "Epoch 15/30, Training Loss: 0.0017981953569687903\n",
      "Running validation...\n",
      "Validation Loss: 0.003376121516339481\n",
      "Validation F1 Score (Entity Level): 0.7415730337078652\n",
      "Best model saved at epoch 15 with F1 score: 0.7415730337078652 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 16/30\n",
      "Epoch 16/30, Training Loss: 0.0014095381654139298\n",
      "Running validation...\n",
      "Validation Loss: 0.003460728912614286\n",
      "Validation F1 Score (Entity Level): 0.7542857142857142\n",
      "Best model saved at epoch 16 with F1 score: 0.7542857142857142 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 17/30\n",
      "Epoch 17/30, Training Loss: 0.0013401346465495105\n",
      "Running validation...\n",
      "Validation Loss: 0.0044123673578724265\n",
      "Validation F1 Score (Entity Level): 0.7630057803468209\n",
      "Best model saved at epoch 17 with F1 score: 0.7630057803468209 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 18/30\n",
      "Epoch 18/30, Training Loss: 0.001388214712884898\n",
      "Running validation...\n",
      "Validation Loss: 0.005068916128948331\n",
      "Validation F1 Score (Entity Level): 0.7126436781609196\n",
      "Training epoch: 19/30\n",
      "Epoch 19/30, Training Loss: 0.001414216846266451\n",
      "Running validation...\n",
      "Validation Loss: 0.004906713555101305\n",
      "Validation F1 Score (Entity Level): 0.7471264367816093\n",
      "Training epoch: 20/30\n",
      "Epoch 20/30, Training Loss: 0.0012593298888532445\n",
      "Running validation...\n",
      "Validation Loss: 0.005463407665956765\n",
      "Validation F1 Score (Entity Level): 0.8342857142857143\n",
      "Best model saved at epoch 20 with F1 score: 0.8342857142857143 as best_model_lr_2.0000000000e-05.pt\n",
      "Training epoch: 21/30\n",
      "Epoch 21/30, Training Loss: 0.0011514193999270599\n",
      "Running validation...\n",
      "Validation Loss: 0.005893068737350404\n",
      "Validation F1 Score (Entity Level): 0.7836257309941521\n",
      "Training epoch: 22/30\n",
      "Epoch 22/30, Training Loss: 0.0011351058880488079\n",
      "Running validation...\n",
      "Validation Loss: 0.005203480890486389\n",
      "Validation F1 Score (Entity Level): 0.7953216374269007\n",
      "Training epoch: 23/30\n",
      "Epoch 23/30, Training Loss: 0.0008898733091579439\n",
      "Running validation...\n",
      "Validation Loss: 0.006607266404898837\n",
      "Validation F1 Score (Entity Level): 0.8070175438596491\n",
      "Training epoch: 24/30\n",
      "Epoch 24/30, Training Loss: 0.0009015057949000038\n",
      "Running validation...\n",
      "Validation Loss: 0.006461801764089614\n",
      "Validation F1 Score (Entity Level): 0.7790697674418606\n",
      "Training epoch: 25/30\n",
      "Epoch 25/30, Training Loss: 0.0009450292479111037\n",
      "Running validation...\n",
      "Validation Loss: 0.005809483991470188\n",
      "Validation F1 Score (Entity Level): 0.8304093567251462\n",
      "Training epoch: 26/30\n",
      "Epoch 26/30, Training Loss: 0.0010495701620432858\n",
      "Running validation...\n",
      "Validation Loss: 0.0058085945493076\n",
      "Validation F1 Score (Entity Level): 0.7999999999999999\n",
      "Training epoch: 27/30\n",
      "Epoch 27/30, Training Loss: 0.001060191222980696\n",
      "Running validation...\n",
      "Validation Loss: 0.00537738628918305\n",
      "Validation F1 Score (Entity Level): 0.8117647058823529\n",
      "Training epoch: 28/30\n",
      "Epoch 28/30, Training Loss: 0.0015018521662568673\n",
      "Running validation...\n",
      "Validation Loss: 0.007925323996460065\n",
      "Validation F1 Score (Entity Level): 0.7586206896551725\n",
      "Training epoch: 29/30\n",
      "Epoch 29/30, Training Loss: 0.0011415559420129284\n",
      "Running validation...\n",
      "Validation Loss: 0.0071654136991128325\n",
      "Validation F1 Score (Entity Level): 0.8160919540229885\n",
      "Training epoch: 30/30\n",
      "Epoch 30/30, Training Loss: 0.0010277927528174284\n",
      "Running validation...\n",
      "Validation Loss: 0.007507522648666054\n",
      "Validation F1 Score (Entity Level): 0.755813953488372\n",
      "Training completed. Best validation F1 score: 0.8342857142857143 at epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_f1</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_f1</td><td>▁▁▄▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇████████▇█▇</td></tr><tr><td>validation_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_validation_f1</td><td>0.83429</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>train_loss</td><td>0.00103</td></tr><tr><td>validation_f1</td><td>0.75581</td></tr><tr><td>validation_loss</td><td>0.00751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sweep-1</strong> at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/ukrvwiw8' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/ukrvwiw8</a><br/> View project at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_113134-ukrvwiw8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k27ruqas with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bizon/example-project2/science_technology/gbpatentdata/wandb/run-20241111_114228-k27ruqas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/k27ruqas' target=\"_blank\">olive-sweep-2</a></strong> to <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/k27ruqas' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/k27ruqas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "Epoch 1/30, Training Loss: 0.4283423125743866\n",
      "Running validation...\n",
      "Validation Loss: 0.17397898063063622\n",
      "Validation F1 Score (Entity Level): 0\n",
      "Training epoch: 2/30\n",
      "Epoch 2/30, Training Loss: 0.0941535122692585\n",
      "Running validation...\n",
      "Validation Loss: 0.05906441621482372\n",
      "Validation F1 Score (Entity Level): 0.24896265560165975\n",
      "Best model saved at epoch 2 with F1 score: 0.24896265560165975 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 3/30\n",
      "Epoch 3/30, Training Loss: 0.03135848937866589\n",
      "Running validation...\n",
      "Validation Loss: 0.017101927427574992\n",
      "Validation F1 Score (Entity Level): 0.738095238095238\n",
      "Best model saved at epoch 3 with F1 score: 0.738095238095238 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 4/30\n",
      "Epoch 4/30, Training Loss: 0.014719743941289684\n",
      "Running validation...\n",
      "Validation Loss: 0.0047556618228554726\n",
      "Validation F1 Score (Entity Level): 0.8048780487804879\n",
      "Best model saved at epoch 4 with F1 score: 0.8048780487804879 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 5/30\n",
      "Epoch 5/30, Training Loss: 0.0050953726943892734\n",
      "Running validation...\n",
      "Validation Loss: 0.00436304256436415\n",
      "Validation F1 Score (Entity Level): 0.8607594936708861\n",
      "Best model saved at epoch 5 with F1 score: 0.8607594936708861 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 6/30\n",
      "Epoch 6/30, Training Loss: 0.0036331022517212355\n",
      "Running validation...\n",
      "Validation Loss: 0.004620775289367884\n",
      "Validation F1 Score (Entity Level): 0.8771929824561402\n",
      "Best model saved at epoch 6 with F1 score: 0.8771929824561402 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 7/30\n",
      "Epoch 7/30, Training Loss: 0.004339056593986849\n",
      "Running validation...\n",
      "Validation Loss: 0.050335717387497425\n",
      "Validation F1 Score (Entity Level): 0.8295454545454545\n",
      "Training epoch: 8/30\n",
      "Epoch 8/30, Training Loss: 0.019479544348238658\n",
      "Running validation...\n",
      "Validation Loss: 0.006016248022206128\n",
      "Validation F1 Score (Entity Level): 0.8192771084337349\n",
      "Training epoch: 9/30\n",
      "Epoch 9/30, Training Loss: 0.004796291555976495\n",
      "Running validation...\n",
      "Validation Loss: 0.004317651648307219\n",
      "Validation F1 Score (Entity Level): 0.9506172839506173\n",
      "Best model saved at epoch 9 with F1 score: 0.9506172839506173 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 10/30\n",
      "Epoch 10/30, Training Loss: 0.00252993480777756\n",
      "Running validation...\n",
      "Validation Loss: 0.008065709058428183\n",
      "Validation F1 Score (Entity Level): 0.9367088607594937\n",
      "Training epoch: 11/30\n",
      "Epoch 11/30, Training Loss: 0.001507610411257095\n",
      "Running validation...\n",
      "Validation Loss: 0.007037912029772997\n",
      "Validation F1 Score (Entity Level): 0.9433962264150944\n",
      "Training epoch: 12/30\n",
      "Epoch 12/30, Training Loss: 0.001550249846332008\n",
      "Running validation...\n",
      "Validation Loss: 0.005981842376058921\n",
      "Validation F1 Score (Entity Level): 0.9375000000000001\n",
      "Training epoch: 13/30\n",
      "Epoch 13/30, Training Loss: 0.0010403872681005548\n",
      "Running validation...\n",
      "Validation Loss: 0.008914813544834033\n",
      "Validation F1 Score (Entity Level): 0.9240506329113924\n",
      "Training epoch: 14/30\n",
      "Epoch 14/30, Training Loss: 0.0010417297986956935\n",
      "Running validation...\n",
      "Validation Loss: 0.007441513109370135\n",
      "Validation F1 Score (Entity Level): 0.9308176100628932\n",
      "Training epoch: 15/30\n",
      "Epoch 15/30, Training Loss: 0.0008455799664564742\n",
      "Running validation...\n",
      "Validation Loss: 0.006317472361843102\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Best model saved at epoch 15 with F1 score: 0.9559748427672956 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 16/30\n",
      "Epoch 16/30, Training Loss: 0.0006341776715999003\n",
      "Running validation...\n",
      "Validation Loss: 0.007029930173303001\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Training epoch: 17/30\n",
      "Epoch 17/30, Training Loss: 0.0004220790876085327\n",
      "Running validation...\n",
      "Validation Loss: 0.007883628146373667\n",
      "Validation F1 Score (Entity Level): 0.9625\n",
      "Best model saved at epoch 17 with F1 score: 0.9625 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 18/30\n",
      "Epoch 18/30, Training Loss: 0.00027448417131381575\n",
      "Running validation...\n",
      "Validation Loss: 0.007845998159609735\n",
      "Validation F1 Score (Entity Level): 0.9625\n",
      "Training epoch: 19/30\n",
      "Epoch 19/30, Training Loss: 0.0002193889619472126\n",
      "Running validation...\n",
      "Validation Loss: 0.008429182700638194\n",
      "Validation F1 Score (Entity Level): 0.9375000000000001\n",
      "Training epoch: 20/30\n",
      "Epoch 20/30, Training Loss: 0.0002835239861269656\n",
      "Running validation...\n",
      "Validation Loss: 0.008847800345392898\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 21/30\n",
      "Epoch 21/30, Training Loss: 0.00047395742240041727\n",
      "Running validation...\n",
      "Validation Loss: 0.008827902609482408\n",
      "Validation F1 Score (Entity Level): 0.9440993788819876\n",
      "Training epoch: 22/30\n",
      "Epoch 22/30, Training Loss: 0.0002578102591238955\n",
      "Running validation...\n",
      "Validation Loss: 0.009132470426266082\n",
      "Validation F1 Score (Entity Level): 0.9620253164556962\n",
      "Training epoch: 23/30\n",
      "Epoch 23/30, Training Loss: 0.0003642196926800049\n",
      "Running validation...\n",
      "Validation Loss: 0.009313721879152581\n",
      "Validation F1 Score (Entity Level): 0.9433962264150944\n",
      "Training epoch: 24/30\n",
      "Epoch 24/30, Training Loss: 0.00016638936131130322\n",
      "Running validation...\n",
      "Validation Loss: 0.008766979197389446\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Training epoch: 25/30\n",
      "Epoch 25/30, Training Loss: 0.00024467038701914134\n",
      "Running validation...\n",
      "Validation Loss: 0.008498498835251667\n",
      "Validation F1 Score (Entity Level): 0.9565217391304348\n",
      "Training epoch: 26/30\n",
      "Epoch 26/30, Training Loss: 0.00019849622973803585\n",
      "Running validation...\n",
      "Validation Loss: 0.008661325893626781\n",
      "Validation F1 Score (Entity Level): 0.9625\n",
      "Training epoch: 27/30\n",
      "Epoch 27/30, Training Loss: 0.00019613805435862255\n",
      "Running validation...\n",
      "Validation Loss: 0.008838382332214678\n",
      "Validation F1 Score (Entity Level): 0.9746835443037974\n",
      "Best model saved at epoch 27 with F1 score: 0.9746835443037974 as best_model_lr_3.0000000000e-05.pt\n",
      "Training epoch: 28/30\n",
      "Epoch 28/30, Training Loss: 0.00019952333665666325\n",
      "Running validation...\n",
      "Validation Loss: 0.008058949839323759\n",
      "Validation F1 Score (Entity Level): 0.9746835443037974\n",
      "Training epoch: 29/30\n",
      "Epoch 29/30, Training Loss: 9.690948210542653e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.008163955702912062\n",
      "Validation F1 Score (Entity Level): 0.9746835443037974\n",
      "Training epoch: 30/30\n",
      "Epoch 30/30, Training Loss: 0.00030814748773385264\n",
      "Running validation...\n",
      "Validation Loss: 0.006668967893347144\n",
      "Validation F1 Score (Entity Level): 0.9375000000000001\n",
      "Training completed. Best validation F1 score: 0.9746835443037974 at epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_f1</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_f1</td><td>▁▃▆▇▇▇▇▇██████████████████████</td></tr><tr><td>validation_loss</td><td>█▃▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>27</td></tr><tr><td>best_validation_f1</td><td>0.97468</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train_loss</td><td>0.00031</td></tr><tr><td>validation_f1</td><td>0.9375</td></tr><tr><td>validation_loss</td><td>0.00667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-2</strong> at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/k27ruqas' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/k27ruqas</a><br/> View project at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_114228-k27ruqas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bq2ytqb2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bizon/example-project2/science_technology/gbpatentdata/wandb/run-20241111_115312-bq2ytqb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/bq2ytqb2' target=\"_blank\">super-sweep-3</a></strong> to <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/bq2ytqb2' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/bq2ytqb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "Epoch 1/30, Training Loss: 0.2876355604579051\n",
      "Running validation...\n",
      "Validation Loss: 0.07656682282686234\n",
      "Validation F1 Score (Entity Level): 0.12269938650306748\n",
      "Best model saved at epoch 1 with F1 score: 0.12269938650306748 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 2/30\n",
      "Epoch 2/30, Training Loss: 0.0544097195379436\n",
      "Running validation...\n",
      "Validation Loss: 0.0294593321159482\n",
      "Validation F1 Score (Entity Level): 0.4433497536945813\n",
      "Best model saved at epoch 2 with F1 score: 0.4433497536945813 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 3/30\n",
      "Epoch 3/30, Training Loss: 0.026865689277959365\n",
      "Running validation...\n",
      "Validation Loss: 0.013947198167443275\n",
      "Validation F1 Score (Entity Level): 0.5513513513513514\n",
      "Best model saved at epoch 3 with F1 score: 0.5513513513513514 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 4/30\n",
      "Epoch 4/30, Training Loss: 0.009920233472560843\n",
      "Running validation...\n",
      "Validation Loss: 0.011154122301377356\n",
      "Validation F1 Score (Entity Level): 0.6404494382022472\n",
      "Best model saved at epoch 4 with F1 score: 0.6404494382022472 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 5/30\n",
      "Epoch 5/30, Training Loss: 0.004354258087308456\n",
      "Running validation...\n",
      "Validation Loss: 0.004564260540064424\n",
      "Validation F1 Score (Entity Level): 0.8888888888888888\n",
      "Best model saved at epoch 5 with F1 score: 0.8888888888888888 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 6/30\n",
      "Epoch 6/30, Training Loss: 0.0021370632312027737\n",
      "Running validation...\n",
      "Validation Loss: 0.0029849831626052037\n",
      "Validation F1 Score (Entity Level): 0.9390243902439025\n",
      "Best model saved at epoch 6 with F1 score: 0.9390243902439025 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 7/30\n",
      "Epoch 7/30, Training Loss: 0.0009921536293404642\n",
      "Running validation...\n",
      "Validation Loss: 0.0026508982336963527\n",
      "Validation F1 Score (Entity Level): 0.920245398773006\n",
      "Training epoch: 8/30\n",
      "Epoch 8/30, Training Loss: 0.0009002729396646222\n",
      "Running validation...\n",
      "Validation Loss: 0.001556185590743553\n",
      "Validation F1 Score (Entity Level): 0.926829268292683\n",
      "Training epoch: 9/30\n",
      "Epoch 9/30, Training Loss: 0.0008362211907903353\n",
      "Running validation...\n",
      "Validation Loss: 0.003913138229108881\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Best model saved at epoch 9 with F1 score: 0.9559748427672956 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 10/30\n",
      "Epoch 10/30, Training Loss: 0.0013681260861631017\n",
      "Running validation...\n",
      "Validation Loss: 0.004717934221844189\n",
      "Validation F1 Score (Entity Level): 0.9440993788819876\n",
      "Training epoch: 11/30\n",
      "Epoch 11/30, Training Loss: 0.0019650971201675325\n",
      "Running validation...\n",
      "Validation Loss: 0.0028643475961871445\n",
      "Validation F1 Score (Entity Level): 0.9221556886227545\n",
      "Training epoch: 12/30\n",
      "Epoch 12/30, Training Loss: 0.003246560530290784\n",
      "Running validation...\n",
      "Validation Loss: 0.005970969941699877\n",
      "Validation F1 Score (Entity Level): 0.9390243902439025\n",
      "Training epoch: 13/30\n",
      "Epoch 13/30, Training Loss: 0.003676312987712057\n",
      "Running validation...\n",
      "Validation Loss: 0.0069900590751785785\n",
      "Validation F1 Score (Entity Level): 0.9681528662420382\n",
      "Best model saved at epoch 13 with F1 score: 0.9681528662420382 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 14/30\n",
      "Epoch 14/30, Training Loss: 0.0018053369130939245\n",
      "Running validation...\n",
      "Validation Loss: 0.006294570441241376\n",
      "Validation F1 Score (Entity Level): 0.8888888888888888\n",
      "Training epoch: 15/30\n",
      "Epoch 15/30, Training Loss: 0.0024380128428068324\n",
      "Running validation...\n",
      "Validation Loss: 0.005786443391116336\n",
      "Validation F1 Score (Entity Level): 0.8606060606060606\n",
      "Training epoch: 16/30\n",
      "Epoch 16/30, Training Loss: 0.0014588539076309341\n",
      "Running validation...\n",
      "Validation Loss: 0.0034650548332137987\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 17/30\n",
      "Epoch 17/30, Training Loss: 0.0008480433674170248\n",
      "Running validation...\n",
      "Validation Loss: 0.002583432346000336\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 18/30\n",
      "Epoch 18/30, Training Loss: 0.0006074612301745219\n",
      "Running validation...\n",
      "Validation Loss: 0.0025682095729280263\n",
      "Validation F1 Score (Entity Level): 0.9259259259259259\n",
      "Training epoch: 19/30\n",
      "Epoch 19/30, Training Loss: 0.0007396582107806656\n",
      "Running validation...\n",
      "Validation Loss: 0.005635812317450473\n",
      "Validation F1 Score (Entity Level): 0.9192546583850932\n",
      "Training epoch: 20/30\n",
      "Epoch 20/30, Training Loss: 0.0006007111678627552\n",
      "Running validation...\n",
      "Validation Loss: 0.004398925055284053\n",
      "Validation F1 Score (Entity Level): 0.9316770186335404\n",
      "Training epoch: 21/30\n",
      "Epoch 21/30, Training Loss: 0.00025891825498547405\n",
      "Running validation...\n",
      "Validation Loss: 0.005486803922394756\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 22/30\n",
      "Epoch 22/30, Training Loss: 6.168488835101016e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.005697511329344707\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Training epoch: 23/30\n",
      "Epoch 23/30, Training Loss: 0.00015039690379126114\n",
      "Running validation...\n",
      "Validation Loss: 0.005918309107983077\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Training epoch: 24/30\n",
      "Epoch 24/30, Training Loss: 0.00023930580649296948\n",
      "Running validation...\n",
      "Validation Loss: 0.0058226290602760855\n",
      "Validation F1 Score (Entity Level): 0.9559748427672956\n",
      "Training epoch: 25/30\n",
      "Epoch 25/30, Training Loss: 8.833571428112919e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.005248542242043186\n",
      "Validation F1 Score (Entity Level): 0.9746835443037974\n",
      "Best model saved at epoch 25 with F1 score: 0.9746835443037974 as best_model_lr_4.0000000000e-05.pt\n",
      "Training epoch: 26/30\n",
      "Epoch 26/30, Training Loss: 0.0003024398264036184\n",
      "Running validation...\n",
      "Validation Loss: 0.004767550741235027\n",
      "Validation F1 Score (Entity Level): 0.9625\n",
      "Training epoch: 27/30\n",
      "Epoch 27/30, Training Loss: 0.0004458956994615922\n",
      "Running validation...\n",
      "Validation Loss: 0.006441719401664159\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 28/30\n",
      "Epoch 28/30, Training Loss: 0.01206749574036318\n",
      "Running validation...\n",
      "Validation Loss: 0.019058484584093094\n",
      "Validation F1 Score (Entity Level): 0.824858757062147\n",
      "Training epoch: 29/30\n",
      "Epoch 29/30, Training Loss: 0.040058854424084224\n",
      "Running validation...\n",
      "Validation Loss: 0.00523803400574252\n",
      "Validation F1 Score (Entity Level): 0.8606060606060606\n",
      "Training epoch: 30/30\n",
      "Epoch 30/30, Training Loss: 0.0029586911987280473\n",
      "Running validation...\n",
      "Validation Loss: 0.00999913172563538\n",
      "Validation F1 Score (Entity Level): 0.8484848484848485\n",
      "Training completed. Best validation F1 score: 0.9746835443037974 at epoch 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_f1</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>validation_f1</td><td>▁▄▅▅▇████████▇▇████████████▇▇▇</td></tr><tr><td>validation_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_validation_f1</td><td>0.97468</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate</td><td>4e-05</td></tr><tr><td>train_loss</td><td>0.00296</td></tr><tr><td>validation_f1</td><td>0.84848</td></tr><tr><td>validation_loss</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-3</strong> at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/bq2ytqb2' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/bq2ytqb2</a><br/> View project at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_115312-bq2ytqb2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xs3tkt6i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bizon/example-project2/science_technology/gbpatentdata/wandb/run-20241111_120402-xs3tkt6i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/xs3tkt6i' target=\"_blank\">dutiful-sweep-4</a></strong> to <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/xs3tkt6i' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/xs3tkt6i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "Epoch 1/30, Training Loss: 0.30421949674685794\n",
      "Running validation...\n",
      "Validation Loss: 0.06956760119646788\n",
      "Validation F1 Score (Entity Level): 0.09815950920245398\n",
      "Best model saved at epoch 1 with F1 score: 0.09815950920245398 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 2/30\n",
      "Epoch 2/30, Training Loss: 0.06370623114829262\n",
      "Running validation...\n",
      "Validation Loss: 0.05183085985481739\n",
      "Validation F1 Score (Entity Level): 0.136986301369863\n",
      "Best model saved at epoch 2 with F1 score: 0.136986301369863 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 3/30\n",
      "Epoch 3/30, Training Loss: 0.026605913570771616\n",
      "Running validation...\n",
      "Validation Loss: 0.01923153386451304\n",
      "Validation F1 Score (Entity Level): 0.3070539419087137\n",
      "Best model saved at epoch 3 with F1 score: 0.3070539419087137 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 4/30\n",
      "Epoch 4/30, Training Loss: 0.01355737952205042\n",
      "Running validation...\n",
      "Validation Loss: 0.01808786834590137\n",
      "Validation F1 Score (Entity Level): 0.34482758620689663\n",
      "Best model saved at epoch 4 with F1 score: 0.34482758620689663 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 5/30\n",
      "Epoch 5/30, Training Loss: 0.008241578257487467\n",
      "Running validation...\n",
      "Validation Loss: 0.008471809444017708\n",
      "Validation F1 Score (Entity Level): 0.574468085106383\n",
      "Best model saved at epoch 5 with F1 score: 0.574468085106383 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 6/30\n",
      "Epoch 6/30, Training Loss: 0.005323429708369076\n",
      "Running validation...\n",
      "Validation Loss: 0.010871168808080256\n",
      "Validation F1 Score (Entity Level): 0.6519337016574586\n",
      "Best model saved at epoch 6 with F1 score: 0.6519337016574586 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 7/30\n",
      "Epoch 7/30, Training Loss: 0.004669915181390631\n",
      "Running validation...\n",
      "Validation Loss: 0.0155322530772537\n",
      "Validation F1 Score (Entity Level): 0.6162162162162163\n",
      "Training epoch: 8/30\n",
      "Epoch 8/30, Training Loss: 0.005045269635350754\n",
      "Running validation...\n",
      "Validation Loss: 0.008003802737221122\n",
      "Validation F1 Score (Entity Level): 0.6966292134831461\n",
      "Best model saved at epoch 8 with F1 score: 0.6966292134831461 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 9/30\n",
      "Epoch 9/30, Training Loss: 0.0027407525243082396\n",
      "Running validation...\n",
      "Validation Loss: 0.009055252594407648\n",
      "Validation F1 Score (Entity Level): 0.6555555555555554\n",
      "Training epoch: 10/30\n",
      "Epoch 10/30, Training Loss: 0.0024092789875188223\n",
      "Running validation...\n",
      "Validation Loss: 0.009264163905754685\n",
      "Validation F1 Score (Entity Level): 0.7912087912087911\n",
      "Best model saved at epoch 10 with F1 score: 0.7912087912087911 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 11/30\n",
      "Epoch 11/30, Training Loss: 0.0020960657032749927\n",
      "Running validation...\n",
      "Validation Loss: 0.007374429202172905\n",
      "Validation F1 Score (Entity Level): 0.6514285714285715\n",
      "Training epoch: 12/30\n",
      "Epoch 12/30, Training Loss: 0.0034700985367332273\n",
      "Running validation...\n",
      "Validation Loss: 0.07214890792965889\n",
      "Validation F1 Score (Entity Level): 0.5463414634146341\n",
      "Training epoch: 13/30\n",
      "Epoch 13/30, Training Loss: 0.018737834742447983\n",
      "Running validation...\n",
      "Validation Loss: 0.06407850980758667\n",
      "Validation F1 Score (Entity Level): 0.5526315789473685\n",
      "Training epoch: 14/30\n",
      "Epoch 14/30, Training Loss: 0.04106080438941717\n",
      "Running validation...\n",
      "Validation Loss: 0.019037954974919558\n",
      "Validation F1 Score (Entity Level): 0.5686274509803921\n",
      "Training epoch: 15/30\n",
      "Epoch 15/30, Training Loss: 0.009460749531475207\n",
      "Running validation...\n",
      "Validation Loss: 0.014694945886731148\n",
      "Validation F1 Score (Entity Level): 0.5824175824175825\n",
      "Training epoch: 16/30\n",
      "Epoch 16/30, Training Loss: 0.006057221015604834\n",
      "Running validation...\n",
      "Validation Loss: 0.006987027125433087\n",
      "Validation F1 Score (Entity Level): 0.6486486486486488\n",
      "Training epoch: 17/30\n",
      "Epoch 17/30, Training Loss: 0.0035386411182116717\n",
      "Running validation...\n",
      "Validation Loss: 0.003689955861773342\n",
      "Validation F1 Score (Entity Level): 0.7640449438202247\n",
      "Training epoch: 18/30\n",
      "Epoch 18/30, Training Loss: 0.005163384422000187\n",
      "Running validation...\n",
      "Validation Loss: 0.0036457146052271128\n",
      "Validation F1 Score (Entity Level): 0.6555555555555554\n",
      "Training epoch: 19/30\n",
      "Epoch 19/30, Training Loss: 0.0031304043028891706\n",
      "Running validation...\n",
      "Validation Loss: 0.003727171686477959\n",
      "Validation F1 Score (Entity Level): 0.7252747252747253\n",
      "Training epoch: 20/30\n",
      "Epoch 20/30, Training Loss: 0.0022203466990807406\n",
      "Running validation...\n",
      "Validation Loss: 0.004629105154890567\n",
      "Validation F1 Score (Entity Level): 0.7570621468926553\n",
      "Training epoch: 21/30\n",
      "Epoch 21/30, Training Loss: 0.002700676462457826\n",
      "Running validation...\n",
      "Validation Loss: 0.004678444529417902\n",
      "Validation F1 Score (Entity Level): 0.8383233532934132\n",
      "Best model saved at epoch 21 with F1 score: 0.8383233532934132 as best_model_lr_5.0000000000e-05.pt\n",
      "Training epoch: 22/30\n",
      "Epoch 22/30, Training Loss: 0.0032002468166562417\n",
      "Running validation...\n",
      "Validation Loss: 0.012953456011018716\n",
      "Validation F1 Score (Entity Level): 0.742857142857143\n",
      "Training epoch: 23/30\n",
      "Epoch 23/30, Training Loss: 0.0046216213959269226\n",
      "Running validation...\n",
      "Validation Loss: 0.014977934944909066\n",
      "Validation F1 Score (Entity Level): 0.745945945945946\n",
      "Training epoch: 24/30\n",
      "Epoch 24/30, Training Loss: 0.005010239139664918\n",
      "Running validation...\n",
      "Validation Loss: 0.00923941214568913\n",
      "Validation F1 Score (Entity Level): 0.544502617801047\n",
      "Training epoch: 25/30\n",
      "Epoch 25/30, Training Loss: 0.005684111655379335\n",
      "Running validation...\n",
      "Validation Loss: 0.00885490095242858\n",
      "Validation F1 Score (Entity Level): 0.5492227979274612\n",
      "Training epoch: 26/30\n",
      "Epoch 26/30, Training Loss: 0.004200243536615744\n",
      "Running validation...\n",
      "Validation Loss: 0.007697691675275564\n",
      "Validation F1 Score (Entity Level): 0.6382978723404256\n",
      "Training epoch: 27/30\n",
      "Epoch 27/30, Training Loss: 0.0028715293641046933\n",
      "Running validation...\n",
      "Validation Loss: 0.005651568702887744\n",
      "Validation F1 Score (Entity Level): 0.6521739130434783\n",
      "Training epoch: 28/30\n",
      "Epoch 28/30, Training Loss: 0.002564130826309944\n",
      "Running validation...\n",
      "Validation Loss: 0.005532905517611653\n",
      "Validation F1 Score (Entity Level): 0.6557377049180327\n",
      "Training epoch: 29/30\n",
      "Epoch 29/30, Training Loss: 0.0026951677524872744\n",
      "Running validation...\n",
      "Validation Loss: 0.006032899429555982\n",
      "Validation F1 Score (Entity Level): 0.6629834254143646\n",
      "Training epoch: 30/30\n",
      "Epoch 30/30, Training Loss: 0.0022372080226584026\n",
      "Running validation...\n",
      "Validation Loss: 0.004963383078575134\n",
      "Validation F1 Score (Entity Level): 0.6304347826086956\n",
      "Training completed. Best validation F1 score: 0.8383233532934132 at epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_f1</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_f1</td><td>▁▁▃▃▆▆▆▇▆█▆▅▅▅▆▆▇▆▇▇█▇▇▅▅▆▆▆▆▆</td></tr><tr><td>validation_loss</td><td>█▆▃▂▁▂▂▁▂▂▁█▇▃▂▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>21</td></tr><tr><td>best_validation_f1</td><td>0.83832</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>train_loss</td><td>0.00224</td></tr><tr><td>validation_f1</td><td>0.63043</td></tr><tr><td>validation_loss</td><td>0.00496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-sweep-4</strong> at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/xs3tkt6i' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/xs3tkt6i</a><br/> View project at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_120402-xs3tkt6i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lnzpusl1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bizon/example-project2/science_technology/gbpatentdata/wandb/run-20241111_121447-lnzpusl1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/lnzpusl1' target=\"_blank\">glowing-sweep-5</a></strong> to <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/sweeps/82ucabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/lnzpusl1' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/lnzpusl1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1/30\n",
      "Epoch 1/30, Training Loss: 0.17031084342549244\n",
      "Running validation...\n",
      "Validation Loss: 0.09834115579724312\n",
      "Validation F1 Score (Entity Level): 0\n",
      "Training epoch: 2/30\n",
      "Epoch 2/30, Training Loss: 0.03463570478682717\n",
      "Running validation...\n",
      "Validation Loss: 0.015824561938643456\n",
      "Validation F1 Score (Entity Level): 0.4479166666666667\n",
      "Best model saved at epoch 2 with F1 score: 0.4479166666666667 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 3/30\n",
      "Epoch 3/30, Training Loss: 0.009205982011432448\n",
      "Running validation...\n",
      "Validation Loss: 0.007575953844934702\n",
      "Validation F1 Score (Entity Level): 0.5978260869565218\n",
      "Best model saved at epoch 3 with F1 score: 0.5978260869565218 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 4/30\n",
      "Epoch 4/30, Training Loss: 0.004529690893832594\n",
      "Running validation...\n",
      "Validation Loss: 0.006565174437128007\n",
      "Validation F1 Score (Entity Level): 0.5604395604395604\n",
      "Training epoch: 5/30\n",
      "Epoch 5/30, Training Loss: 0.0030462495051324368\n",
      "Running validation...\n",
      "Validation Loss: 0.006493053399026394\n",
      "Validation F1 Score (Entity Level): 0.6741573033707866\n",
      "Best model saved at epoch 5 with F1 score: 0.6741573033707866 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 6/30\n",
      "Epoch 6/30, Training Loss: 0.0022235312208067626\n",
      "Running validation...\n",
      "Validation Loss: 0.003978282155003399\n",
      "Validation F1 Score (Entity Level): 0.61139896373057\n",
      "Training epoch: 7/30\n",
      "Epoch 7/30, Training Loss: 0.03188074602318617\n",
      "Running validation...\n",
      "Validation Loss: 0.04518730100244284\n",
      "Validation F1 Score (Entity Level): 0.28292682926829266\n",
      "Training epoch: 8/30\n",
      "Epoch 8/30, Training Loss: 0.015812898403964937\n",
      "Running validation...\n",
      "Validation Loss: 0.016534093534573913\n",
      "Validation F1 Score (Entity Level): 0.42105263157894735\n",
      "Training epoch: 9/30\n",
      "Epoch 9/30, Training Loss: 0.011545570412029823\n",
      "Running validation...\n",
      "Validation Loss: 0.014434045879170299\n",
      "Validation F1 Score (Entity Level): 0.3660714285714286\n",
      "Training epoch: 10/30\n",
      "Epoch 10/30, Training Loss: 0.011971777110981444\n",
      "Running validation...\n",
      "Validation Loss: 0.019166137324646115\n",
      "Validation F1 Score (Entity Level): 0.28571428571428575\n",
      "Training epoch: 11/30\n",
      "Epoch 11/30, Training Loss: 0.005236493763125812\n",
      "Running validation...\n",
      "Validation Loss: 0.00971245119580999\n",
      "Validation F1 Score (Entity Level): 0.7325581395348836\n",
      "Best model saved at epoch 11 with F1 score: 0.7325581395348836 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 12/30\n",
      "Epoch 12/30, Training Loss: 0.0030283366795629263\n",
      "Running validation...\n",
      "Validation Loss: 0.004411522335431073\n",
      "Validation F1 Score (Entity Level): 0.9554140127388535\n",
      "Best model saved at epoch 12 with F1 score: 0.9554140127388535 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 13/30\n",
      "Epoch 13/30, Training Loss: 0.0017071848148286033\n",
      "Running validation...\n",
      "Validation Loss: 0.004076619137777016\n",
      "Validation F1 Score (Entity Level): 0.9135802469135801\n",
      "Training epoch: 14/30\n",
      "Epoch 14/30, Training Loss: 0.0013411773416009964\n",
      "Running validation...\n",
      "Validation Loss: 0.0034350948335486464\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 15/30\n",
      "Epoch 15/30, Training Loss: 0.0005682144953122285\n",
      "Running validation...\n",
      "Validation Loss: 0.003961812559282407\n",
      "Validation F1 Score (Entity Level): 0.9685534591194969\n",
      "Best model saved at epoch 15 with F1 score: 0.9685534591194969 as best_model_lr_6.0000000000e-05.pt\n",
      "Training epoch: 16/30\n",
      "Epoch 16/30, Training Loss: 0.0008931539271846608\n",
      "Running validation...\n",
      "Validation Loss: 0.00372453893214697\n",
      "Validation F1 Score (Entity Level): 0.9565217391304348\n",
      "Training epoch: 17/30\n",
      "Epoch 17/30, Training Loss: 0.00033301163269546424\n",
      "Running validation...\n",
      "Validation Loss: 0.0031947000961736194\n",
      "Validation F1 Score (Entity Level): 0.9390243902439025\n",
      "Training epoch: 18/30\n",
      "Epoch 18/30, Training Loss: 0.0004960379145207602\n",
      "Running validation...\n",
      "Validation Loss: 0.0024143214268406155\n",
      "Validation F1 Score (Entity Level): 0.9382716049382716\n",
      "Training epoch: 19/30\n",
      "Epoch 19/30, Training Loss: 0.0007840065410770573\n",
      "Running validation...\n",
      "Validation Loss: 0.0010346043072786415\n",
      "Validation F1 Score (Entity Level): 0.9565217391304348\n",
      "Training epoch: 20/30\n",
      "Epoch 20/30, Training Loss: 0.0003359797068090605\n",
      "Running validation...\n",
      "Validation Loss: 0.0011863530235132203\n",
      "Validation F1 Score (Entity Level): 0.9625\n",
      "Training epoch: 21/30\n",
      "Epoch 21/30, Training Loss: 0.00018462198416576334\n",
      "Running validation...\n",
      "Validation Loss: 0.0021979221392030013\n",
      "Validation F1 Score (Entity Level): 0.9440993788819876\n",
      "Training epoch: 22/30\n",
      "Epoch 22/30, Training Loss: 0.0006312022336108688\n",
      "Running validation...\n",
      "Validation Loss: 0.002722311153775081\n",
      "Validation F1 Score (Entity Level): 0.8987341772151899\n",
      "Training epoch: 23/30\n",
      "Epoch 23/30, Training Loss: 0.001050145692715887\n",
      "Running validation...\n",
      "Validation Loss: 0.0029352393466979265\n",
      "Validation F1 Score (Entity Level): 0.8\n",
      "Training epoch: 24/30\n",
      "Epoch 24/30, Training Loss: 0.0012727606056917768\n",
      "Running validation...\n",
      "Validation Loss: 0.002855087943316903\n",
      "Validation F1 Score (Entity Level): 0.9375000000000001\n",
      "Training epoch: 25/30\n",
      "Epoch 25/30, Training Loss: 0.00028126557284243364\n",
      "Running validation...\n",
      "Validation Loss: 0.0031126788544497686\n",
      "Validation F1 Score (Entity Level): 0.9259259259259259\n",
      "Training epoch: 26/30\n",
      "Epoch 26/30, Training Loss: 3.766404021613804e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.003854180710050059\n",
      "Validation F1 Score (Entity Level): 0.95\n",
      "Training epoch: 27/30\n",
      "Epoch 27/30, Training Loss: 4.889369415650435e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.004023017723852718\n",
      "Validation F1 Score (Entity Level): 0.9685534591194969\n",
      "Training epoch: 28/30\n",
      "Epoch 28/30, Training Loss: 3.116980064987729e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.004003280552069555\n",
      "Validation F1 Score (Entity Level): 0.9685534591194969\n",
      "Training epoch: 29/30\n",
      "Epoch 29/30, Training Loss: 8.527646722692832e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.004222513199692912\n",
      "Validation F1 Score (Entity Level): 0.9685534591194969\n",
      "Training epoch: 30/30\n",
      "Epoch 30/30, Training Loss: 6.8201628891984e-05\n",
      "Running validation...\n",
      "Validation Loss: 0.0032092455352312754\n",
      "Validation F1 Score (Entity Level): 0.9440993788819876\n",
      "Training completed. Best validation F1 score: 0.9685534591194969 at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>▁</td></tr><tr><td>best_validation_f1</td><td>▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_f1</td><td>▁▄▅▅▆▅▃▄▄▃▆██████████▇▇███████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_validation_f1</td><td>0.96855</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>train_loss</td><td>7e-05</td></tr><tr><td>validation_f1</td><td>0.9441</td></tr><tr><td>validation_loss</td><td>0.00321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-sweep-5</strong> at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/lnzpusl1' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title/runs/lnzpusl1</a><br/> View project at: <a href='https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title' target=\"_blank\">https://wandb.ai/matthewleechen/gbpatentdata_grid_xlm_title</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_121447-lnzpusl1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "# RUN HYPERPARAM SWEEP\n",
    "wandb.agent(sweep_id, function=sweep_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a24355-d516-4af5-a281-e470c6cb24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THEN USE BEST MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8daff4-cf0a-4e0d-985d-5cb838c79f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(f1_scores):\n",
    "    print(\"Classification Report (Entity Level):\")\n",
    "    print(\"{:>15} {:>10} {:>10} {:>10} {:>10}\".format(\"Entity\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"))\n",
    "    for entity, scores in f1_scores.items():\n",
    "        print(\"{:>15} {:>10.4f} {:>10.4f} {:>10.4f} {:>10}\".format(\n",
    "            entity,\n",
    "            scores[\"precision\"],\n",
    "            scores[\"recall\"],\n",
    "            scores[\"f1-score\"],\n",
    "            scores[\"support\"]\n",
    "        ))\n",
    "\n",
    "def save_classification_report(f1_scores, filename=\"classification_report.csv\"):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Entity', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for entity, scores in f1_scores.items():\n",
    "            writer.writerow({\n",
    "                'Entity': entity,\n",
    "                'Precision': f\"{scores['precision']:.4f}\",\n",
    "                'Recall': f\"{scores['recall']:.4f}\",\n",
    "                'F1-Score': f\"{scores['f1-score']:.4f}\",\n",
    "                'Support': scores['support']\n",
    "            })\n",
    "    print(f\"Classification report saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f520d586-6519-493f-9275-37af5c714e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate from grid search: 4.5e-05\n",
      "Loading best model from best_model_lr_6.5000000000e-05.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 1.22 GiB already allocated; 5.94 MiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: default_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[0;32m---> 28\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(model, dataloader):\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/example-project2/env/lib/python3.10/site-packages/torch/_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 1.22 GiB already allocated; 5.94 MiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Fetch the best run from W&B\n",
    "api = wandb.Api()\n",
    "sweep_runs = api.runs(path=\"matthewleechen/gbpatentdata_grid_xlm_title\")  # Replace with your W&B username/project\n",
    "\n",
    "# Find the run with the highest best_validation_f1\n",
    "best_run = None\n",
    "best_f1 = -1\n",
    "\n",
    "for run in sweep_runs:\n",
    "    if run.state == 'finished':\n",
    "        val_f1 = run.summary.get('best_validation_f1')\n",
    "        if val_f1 is not None and val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_run = run\n",
    "\n",
    "if best_run is not None:\n",
    "    best_lr = best_run.config['learning_rate']\n",
    "    print(f\"Best learning rate from grid search: {best_lr}\")\n",
    "else:\n",
    "    print(\"No completed runs found.\")\n",
    "    best_lr = default_parameters['LEARNING_RATE']\n",
    "\n",
    "# Load the best model\n",
    "lr_str = f\"6.5000000000e-05\"\n",
    "best_model_path = f'best_model_lr_{lr_str}.pt'\n",
    "print(f\"Loading best model from {best_model_path}\")\n",
    "model = prepare_model({'model_name': default_parameters['model_name']})\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    output = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Ground truth labels\n",
    "            offsets = batch['offset_mapping']\n",
    "            texts = batch['texts']\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            for i in range(input_ids.size(0)):\n",
    "                # Move tensors to CPU\n",
    "                input_ids_i = input_ids[i].cpu()\n",
    "                pred_labels_i = preds[i].cpu()\n",
    "                true_labels_i = labels[i].cpu()\n",
    "                attention_mask_i = attention_mask[i].cpu()\n",
    "                offset_mapping_i = offsets[i]\n",
    "                text = texts[i]\n",
    "\n",
    "                active_tokens = attention_mask_i == 1\n",
    "                input_ids_i = input_ids_i[active_tokens].numpy()\n",
    "                pred_labels_i = pred_labels_i[active_tokens].numpy()\n",
    "                true_labels_i = true_labels_i[active_tokens].numpy()\n",
    "                offset_mapping_i = offset_mapping_i[active_tokens].numpy()\n",
    "\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids_i)\n",
    "                pred_tags = [id2label[label_id] if label_id != -100 else 'O' for label_id in pred_labels_i]\n",
    "                true_tags = [id2label[label_id] if label_id != -100 else 'O' for label_id in true_labels_i]\n",
    "\n",
    "                pred_entities = reconstruct_entities(tokens, pred_tags, offset_mapping_i)\n",
    "                true_entities = reconstruct_entities(tokens, true_tags, offset_mapping_i)\n",
    "\n",
    "                # Ensure that 'ground_truth_entities' is included\n",
    "                output.append({\n",
    "                    \"id\": len(output),\n",
    "                    \"sentence\": text,\n",
    "                    \"predicted_entities\": pred_entities,\n",
    "                    \"ground_truth_entities\": true_entities\n",
    "                })\n",
    "\n",
    "    with open(\"test_set_predictions.json\", \"w\") as f:\n",
    "        json.dump(output, f, indent=4)\n",
    "    print(\"Predictions saved to test_set_predictions.json\")\n",
    "\n",
    "# Run prediction on test set\n",
    "predict(model, testing_loader)\n",
    "\n",
    "# Load predictions and compute metrics\n",
    "with open(\"test_set_predictions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "true_entities_list = [item['ground_truth_entities'] for item in data]\n",
    "pred_entities_list = [item['predicted_entities'] for item in data]\n",
    "\n",
    "f1_scores = compute_f1_scores(true_entities_list, pred_entities_list)\n",
    "print_classification_report(f1_scores)\n",
    "\n",
    "# Optionally, save the classification report\n",
    "save_classification_report(f1_scores, filename=f\"classification_report_lr_{lr_str}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00941ba6-4697-4fba-bd2e-ab1c6bc96c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b66802b2024bbbad944bee5c27a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89df00b013340a598bf0d5a42add127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba81a932620f4537b7e2647694b2c670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdea2a77e4e436e90b0d983fbb81b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ab222897be4c698cb97c795824a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gbpatentdata/patent_data_ner_xlmroberta_20241110/commit/95c4820e160bbaaaa5f9fe94e7a2a49a9e1f42ae', commit_message='Upload XLMRobertaForTokenClassification', commit_description='', oid='95c4820e160bbaaaa5f9fe94e7a2a49a9e1f42ae', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model to hub\n",
    "model_name = \"matthewleechen/patent_entities_ner\"\n",
    "\n",
    "# Upload files to the hub\n",
    "tokenizer.push_to_hub(model_name)\n",
    "model.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e4901-0d5e-4149-8f2e-6f32f68844ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
